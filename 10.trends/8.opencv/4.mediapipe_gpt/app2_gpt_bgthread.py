from flask import Flask, render_template, Response, jsonify
import cv2
import mediapipe as mp
import time
import atexit
import os
from dotenv import load_dotenv
import openai
import threading
from datetime import datetime

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° OpenAI ì´ˆê¸°í™”
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")
client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

app = Flask(__name__, static_folder='static')

# MediaPipe ì´ˆê¸°í™”
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)
mp_drawing = mp.solutions.drawing_utils

# ì „ì—­ ë³€ìˆ˜
cap = cv2.VideoCapture(0)
if not cap.isOpened():
    print("âŒ ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit(1)

latest_pose_summary = "No pose detected yet."
latest_gpt_text = "ìì„¸ ë¶„ì„ì„ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘..."
latest_gpt_based_on_time = None  # ë¶„ì„ ê¸°ì¤€ ì‹œê° (í¬ì¦ˆê°€ ìˆ˜ì§‘ëœ ì‹œê°)
latest_joints_data = {}
last_api_call_time = 0


def summarize_pose(landmarks, frame_width=640, frame_height=480):
    global latest_joints_data, latest_gpt_based_on_time
    key_joints = {
        "left_shoulder": landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER],
        "right_shoulder": landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER],
        "left_elbow": landmarks[mp_pose.PoseLandmark.LEFT_ELBOW],
        "right_elbow": landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW],
        "left_wrist": landmarks[mp_pose.PoseLandmark.LEFT_WRIST],
        "right_wrist": landmarks[mp_pose.PoseLandmark.RIGHT_WRIST],
        "left_hip": landmarks[mp_pose.PoseLandmark.LEFT_HIP],
        "right_hip": landmarks[mp_pose.PoseLandmark.RIGHT_HIP],
    }

    summary = []
    visible_parts = 0
    joints_data = {}

    for name, lm in key_joints.items():
        if lm.visibility > 0.5:
            pixel_x = int(lm.x * frame_width)
            pixel_y = int(lm.y * frame_height)
            summary.append(f"{name}: ì •ê·œí™”({lm.x:.2f}, {lm.y:.2f}) / í”½ì…€({pixel_x}, {pixel_y})")
            visible_parts += 1
            joints_data[name] = {
                'x': float(f"{lm.x:.2f}"),
                'y': float(f"{lm.y:.2f}"),
                'visibility': float(f"{lm.visibility:.2f}")
            }
        else:
            summary.append(f"{name}: not visible")

    if visible_parts < 4:
        return "ê°ì§€ëœ ê´€ì ˆì´ ë¶€ì¡±í•©ë‹ˆë‹¤.", {}

    latest_joints_data = joints_data
    latest_gpt_based_on_time = datetime.now()  # ì´ ì‹œì ì˜ í¬ì¦ˆë¥¼ ê¸°ì¤€ìœ¼ë¡œ GPT ë¶„ì„í•˜ê²Œ ë¨
    return "\n".join(summary), joints_data


def get_gpt_feedback(joints_data):
    prompt = f"""ë‹¤ìŒì€ ì‚¬ëŒì˜ í¬ì¦ˆì— ëŒ€í•œ ì •ê·œí™”ëœ ì¢Œí‘œ ë°ì´í„°ì…ë‹ˆë‹¤:
{joints_data}

ì´ ì¢Œí‘œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ëŒì˜ ìì„¸ë‚˜ ìœ„ì¹˜ë¥¼ ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
íŠ¹íˆ íŒ”ì˜ ìœ„ì¹˜(ë“¤ì—ˆëŠ”ì§€, ë‚´ë ¸ëŠ”ì§€, ì•ì´ë‚˜ ì˜†ìœ¼ë¡œ ë»—ì—ˆëŠ”ì§€)ì— ì£¼ëª©í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ê°€ëŠ¥í•˜ë‹¤ë©´:
1. í˜„ì¬ ìì„¸ê°€ ì–´ë–¤ ìì„¸ì¸ì§€
2. ì–‘íŒ”ì˜ ìœ„ì¹˜ì™€ ì›€ì§ì„
3. ìì„¸ì˜ ê· í˜•ì´ë‚˜ ì •ë ¬ ìƒíƒœ

ì‘ë‹µì€ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ í•´ì£¼ì„¸ìš”."""

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=150
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"GPT API ì˜¤ë¥˜: {e}")
        return f"ë¶„ì„ ì‹¤íŒ¨: {str(e)[:100]}"


def gpt_analysis_loop():
    global latest_joints_data, latest_gpt_text, last_api_call_time
    while True:
        time.sleep(1)
        if latest_joints_data and time.time() - last_api_call_time > 5:
            print(f"Query GPT...")
            latest_gpt_text = get_gpt_feedback(latest_joints_data)
            last_api_call_time = time.time()
            
            # ë¶„ì„ì´ ëë‚¬ìœ¼ë¯€ë¡œ ì´ì „ ë°ì´í„° ì´ˆê¸°í™”
            latest_joints_data = {}


def generate_frames():
    global latest_pose_summary
    while True:
        success, frame = cap.read()
        if not success:
            print("âš ï¸ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
            break

        frame_height, frame_width = frame.shape[:2]
        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = pose.process(image)
        annotated_frame = frame.copy()

        if results.pose_landmarks:
            mp_drawing.draw_landmarks(
                annotated_frame, results.pose_landmarks,
                mp_pose.POSE_CONNECTIONS,
                mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2),
                mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2)
            )
            summary, _ = summarize_pose(results.pose_landmarks.landmark, frame_width, frame_height)
            latest_pose_summary = summary
        else:
            latest_pose_summary = "í¬ì¦ˆê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        ret, buffer = cv2.imencode('.jpg', annotated_frame)
        frame = buffer.tobytes()

        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' + frame + b'\r\n')


@app.route('/')
def index():
    return render_template('index2.html')


@app.route('/video_feed')
def video_feed():
    return Response(generate_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')


@app.route('/pose_summary')
def pose_summary():
    based_on_time = latest_gpt_based_on_time.isoformat() if latest_gpt_based_on_time else None

    return jsonify({
        "summary": latest_pose_summary,
        "description": latest_gpt_text,
        "based_on_time": based_on_time
    })


@app.route('/shutdown')
def shutdown():
    cap.release()
    return "Camera Released"


@atexit.register
def release_camera_on_exit():
    if cap.isOpened():
        print("ğŸ”’ ì¢…ë£Œ: ì¹´ë©”ë¼ í•´ì œ")
        cap.release()


if __name__ == '__main__':
    threading.Thread(target=gpt_analysis_loop, daemon=True).start()
    app.run(debug=True, use_reloader=False, host='0.0.0.0')
