from flask import Flask, render_template, Response, jsonify
import cv2
import mediapipe as mp
import time
import atexit
import os
from dotenv import load_dotenv
import openai

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

app = Flask(__name__, static_folder='static')

# MediaPipe ì´ˆê¸°í™”
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)
mp_drawing = mp.solutions.drawing_utils

# ì›¹ìº  ì´ˆê¸°í™”
cap = cv2.VideoCapture(0)
latest_pose_summary = "No pose detected yet."
latest_gpt_text = "ìì„¸ ë¶„ì„ì„ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘..."
last_api_call_time = 0  # API í˜¸ì¶œ ì‹œê°„ ì¶”ì 

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


def summarize_pose(landmarks, frame_width=640, frame_height=480):
    """ê´€ì ˆ ì¢Œí‘œë¥¼ ìš”ì•½í•˜ê³  ì •ê·œí™”ëœ ì¢Œí‘œì™€ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜"""
    key_joints = {
        # ìƒì²´ ê´€ì ˆ
        "left_shoulder": landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER],
        "right_shoulder": landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER],
        "left_elbow": landmarks[mp_pose.PoseLandmark.LEFT_ELBOW],
        "right_elbow": landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW],
        "left_wrist": landmarks[mp_pose.PoseLandmark.LEFT_WRIST],
        "right_wrist": landmarks[mp_pose.PoseLandmark.RIGHT_WRIST],
        # ì† ê´€ì ˆ
        "left_pinky": landmarks[mp_pose.PoseLandmark.LEFT_PINKY],
        "right_pinky": landmarks[mp_pose.PoseLandmark.RIGHT_PINKY],
        "left_index": landmarks[mp_pose.PoseLandmark.LEFT_INDEX],
        "right_index": landmarks[mp_pose.PoseLandmark.RIGHT_INDEX],
        "left_thumb": landmarks[mp_pose.PoseLandmark.LEFT_THUMB],
        "right_thumb": landmarks[mp_pose.PoseLandmark.RIGHT_THUMB],
        # í•˜ì²´ ê´€ì ˆ
        "left_hip": landmarks[mp_pose.PoseLandmark.LEFT_HIP],
        "right_hip": landmarks[mp_pose.PoseLandmark.RIGHT_HIP],
    }

    summary = []
    visible_parts = 0
    
    for name, lm in key_joints.items():
        if lm.visibility > 0.5:  # visibility ì„ê³„ê°’ ì¶”ê°€
            # ì •ê·œí™”ëœ ì¢Œí‘œë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
            pixel_x = int(lm.x * frame_width)
            pixel_y = int(lm.y * frame_height)
            summary.append(f"{name}: ì •ê·œí™”({lm.x:.2f}, {lm.y:.2f}) / í”½ì…€({pixel_x}, {pixel_y})")
            visible_parts += 1
        else:
            summary.append(f"{name}: not visible")
    
    if visible_parts < 4:  # ìµœì†Œ 4ê°œì˜ ê´€ì ˆì´ ë³´ì—¬ì•¼ í•¨
        return "ì£¼ìš” ì‹ ì²´ ë¶€ìœ„ê°€ ì¶©ë¶„íˆ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì „ì‹ ì´ ë³´ì´ë„ë¡ ì¹´ë©”ë¼ ìœ„ì¹˜ë¥¼ ì¡°ì •í•´ì£¼ì„¸ìš”.", {}
    
    # GPTì— ë³´ë‚¼ ê´€ì ˆ ë°ì´í„° (ì •ê·œí™”ëœ ì¢Œí‘œë§Œ)
    gpt_joints_data = {}
    for name, lm in key_joints.items():
        if lm.visibility > 0.5:
            gpt_joints_data[name] = {'x': float(f"{lm.x:.2f}"), 'y': float(f"{lm.y:.2f}"), 'visibility': float(f"{lm.visibility:.2f}")}
    
    return "\n".join(summary), gpt_joints_data


def get_gpt_feedback(joints_data):
    """OpenAI GPTì— í¬ì¦ˆë¥¼ ë¶„ì„í•˜ë„ë¡ ìš”ì²­"""
    if not joints_data:
        return "ì¶©ë¶„í•œ ê´€ì ˆ ë°ì´í„°ê°€ ì—†ì–´ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    prompt = f"""ë‹¤ìŒì€ ì‚¬ëŒì˜ í¬ì¦ˆì— ëŒ€í•œ ì •ê·œí™”ëœ ì¢Œí‘œ ë°ì´í„°ì…ë‹ˆë‹¤:
{joints_data}

ì´ ì¢Œí‘œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ëŒì˜ ìì„¸ë‚˜ ìœ„ì¹˜ë¥¼ ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
íŠ¹íˆ íŒ”ì˜ ìœ„ì¹˜(ë“¤ì—ˆëŠ”ì§€, ë‚´ë ¸ëŠ”ì§€, ì•ì´ë‚˜ ì˜†ìœ¼ë¡œ ë»—ì—ˆëŠ”ì§€)ì— ì£¼ëª©í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ê°€ëŠ¥í•˜ë‹¤ë©´:
1. í˜„ì¬ ìì„¸ê°€ ì–´ë–¤ ìì„¸ì¸ì§€ (ì„œìˆëŠ”ì§€, ì•‰ì•„ìˆëŠ”ì§€, íŒ”ì„ ë“¤ê³  ìˆëŠ”ì§€ ë“±)
2. ì–‘íŒ”ì˜ ìœ„ì¹˜ì™€ ì›€ì§ì„ (ì™¼íŒ”/ì˜¤ë¥¸íŒ”ì„ ì–¼ë§ˆë‚˜ ë“¤ì—ˆëŠ”ì§€, ë°©í–¥ì€ ì–´ë–¤ì§€)
3. ìì„¸ì˜ ê· í˜•ì´ë‚˜ ì •ë ¬ ìƒíƒœ

ì‘ë‹µì€ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ í•´ì£¼ì„¸ìš”."""

    try:
        response = client.chat.completions.create(
            model="gpt-4o",  # ë˜ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ ë‹¤ë¥¸ ëª¨ë¸
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=150
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"GPT API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return f"ìì„¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)[:100]}..."


def generate_frames():
    """ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ìƒì„±í•˜ê³  í¬ì¦ˆë¥¼ ê°ì§€í•˜ì—¬ ë¶„ì„"""
    global latest_pose_summary, latest_gpt_text, last_api_call_time
    
    while True:
        success, frame = cap.read()
        if not success:
            break

        # ì¢Œìš° ë°˜ì „ (ê±°ìš¸ì²˜ëŸ¼ ë³´ì´ë„ë¡) - ì¢Œí‘œê°’ ì¢Œìš° ë°”ê¿”ì„œ í• ë‹¹í•´ì•¼í•¨
        # frame = cv2.flip(frame, 1)
        
        # í”„ë ˆì„ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
        frame_height, frame_width = frame.shape[:2]
        
        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = pose.process(image)

        annotated_frame = frame.copy()
        if results.pose_landmarks:
            mp_drawing.draw_landmarks(
                annotated_frame, 
                results.pose_landmarks, 
                mp_pose.POSE_CONNECTIONS,
                mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),
                mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2)
            )

            # GPT ë¶„ì„ ì—…ë°ì´íŠ¸ (5ì´ˆë§ˆë‹¤)
            now = time.time()
            if now - last_api_call_time > 5:  # 5ì´ˆë§ˆë‹¤ GPT API í˜¸ì¶œ
                pose_summary, joints_data = summarize_pose(results.pose_landmarks.landmark, frame_width, frame_height)
                latest_pose_summary = pose_summary
                
                if joints_data:  # ì¶©ë¶„í•œ ê´€ì ˆ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ GPT í˜¸ì¶œ
                    latest_gpt_text = get_gpt_feedback(joints_data)
                    last_api_call_time = now
        else:
            latest_pose_summary = "í¬ì¦ˆê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        ret, buffer = cv2.imencode('.jpg', annotated_frame)
        frame = buffer.tobytes()

        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' + frame + b'\r\n')


@app.route('/')
def index():
    return render_template('index.html')


@app.route('/video_feed')
def video_feed():
    return Response(generate_frames(),
                    mimetype='multipart/x-mixed-replace; boundary=frame')


@app.route('/pose_summary')
def pose_summary():
    return jsonify({
        "summary": latest_pose_summary,
        "description": latest_gpt_text
    })


@app.route('/shutdown')
def shutdown():
    cap.release()
    return "Camera Released"


@atexit.register
def release_camera_on_exit():
    if cap.isOpened():
        print("ğŸ”’ Cleaning up: Releasing webcam...")
        cap.release()
        print("âœ… Webcam released.")


if __name__ == '__main__':
    app.run(debug=True, use_reloader=False, host='0.0.0.0')
