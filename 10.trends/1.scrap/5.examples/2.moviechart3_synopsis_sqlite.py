import os
import re
import sqlite3
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, urljoin

# ì„¤ì •
BASE_URL = "https://www.moviechart.co.kr"
TARGET_URL = BASE_URL + "/rank/realtime/index/image"
HEADERS = {"User-Agent": "Mozilla/5.0"}
DB_PATH = "moviechart.db"
THUMBNAIL_DIR = "thumbnails"

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(THUMBNAIL_DIR, exist_ok=True)

# íŒŒì¼ëª… ì •ë¦¬ í•¨ìˆ˜
def sanitize_filename(name):
    return re.sub(r'[\\/*?:"<>|]', '_', name)

# ì‹œë†‰ì‹œìŠ¤ ê°€ì ¸ì˜¤ê¸°
def get_synopsis(detail_url):
    try:
        res = requests.get(detail_url, headers=HEADERS)
        if res.status_code == 200:
            soup = BeautifulSoup(res.text, "html.parser")
            box = soup.select_one("div.contentBox div.text")
            return box.text.strip() if box else ''
    except Exception as e:
        print(f"[!] ì‹œë†‰ì‹œìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {detail_url} - {e}")
    return ''

# DB ì„¤ì • ë° í…Œì´ë¸” ìƒì„±
def setup_database():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute('''
    CREATE TABLE IF NOT EXISTS movies (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        rank INTEGER,
        title TEXT UNIQUE,
        detail_link TEXT,
        thumbnail_path TEXT,
        original_url TEXT,
        synopsis TEXT,
        scraped_at DATETIME DEFAULT CURRENT_TIMESTAMP
    )
    ''')
    conn.commit()
    return conn, cur

# ì˜í™” ëª©ë¡ ìŠ¤í¬ë˜í•‘
def scrape_movies():
    response = requests.get(TARGET_URL, headers=HEADERS)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")
    movie_cards = soup.select('div.movieBox li.movieBox-item')
    print(f"ğŸï¸ ë¬´ë¹„ì¹´ë“œ ê°œìˆ˜: {len(movie_cards)}")
    return movie_cards

# ì˜í™” 1ê°œ ì €ì¥
def process_movie(card, idx, cur):
    title_tag = card.select_one('div.movie-title h3')
    img_tag = card.select_one('img')
    link_tag = card.select_one('a')

    title = title_tag.text.strip() if title_tag else "ì œëª©ì—†ìŒ"
    detail_link = urljoin(BASE_URL, link_tag['href']) if link_tag and link_tag.has_attr('href') else ""
    thumb_url = img_tag['src'] if img_tag and img_tag.has_attr('src') else ""

    # ì›ë³¸ ì´ë¯¸ì§€ URL ì¶”ì¶œ
    parsed = urlparse(thumb_url)
    query = parse_qs(parsed.query)
    original_url = query.get('source', [''])[0]

    # ì¸ë„¤ì¼ ì €ì¥
    thumbnail_path = ''
    try:
        if thumb_url.startswith('/'):
            thumb_url = urljoin(BASE_URL, thumb_url)
        image_data = requests.get(thumb_url, headers=HEADERS).content
        if len(image_data) > 0:
            filename = f"{THUMBNAIL_DIR}/{idx}_{sanitize_filename(title)}.jpg"
            with open(filename, 'wb') as f:
                f.write(image_data)
            thumbnail_path = filename
        else:
            print(f"[!] ì¸ë„¤ì¼ ì—†ìŒ (0ë°”ì´íŠ¸): {title}")
    except Exception as e:
        print(f"[!] ì¸ë„¤ì¼ ì €ì¥ ì‹¤íŒ¨: {title} - {e}")

    # ì‹œë†‰ì‹œìŠ¤ ê°€ì ¸ì˜¤ê¸°
    synopsis = get_synopsis(detail_link)

    # ì¶œë ¥
    print(f"{idx:2}. {title}")
    print(f"    ì¸ë„¤ì¼: {thumbnail_path}")
    print(f"    ì›ë³¸ ì´ë¯¸ì§€: {original_url}")
    print(f"    ìƒì„¸ ë§í¬: {detail_link}")
    print(f"    ì‹œë†‰ì‹œìŠ¤: {synopsis[:100]}...") # 100ê¸€ìë§Œ ì¶œë ¥

    # DB ì‚½ì…
    try:
        cur.execute('''
        INSERT OR IGNORE INTO movies (rank, title, detail_link, thumbnail_path, original_url, synopsis)
        VALUES (?, ?, ?, ?, ?, ?)
        ''', (idx, title, detail_link, thumbnail_path, original_url, synopsis))
    except Exception as e:
        print(f"[DB ì˜¤ë¥˜] {title}: {e}")

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    conn, cur = setup_database()
    cards = scrape_movies()
    for idx, card in enumerate(cards, start=1):
        process_movie(card, idx, cur)
    conn.commit()
    conn.close()

# ì‹¤í–‰
if __name__ == "__main__":
    main()
